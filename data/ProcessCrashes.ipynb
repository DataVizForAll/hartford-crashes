{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Process Crash Tables to CSV and JSON\n",
    "\n",
    "Use `Data Query Tool` to create a new export from UCONN Crashes\n",
    "https://www.ctcrash.uconn.edu/QueryTool2.action\n",
    "\n",
    "* Since the tool does not let you export large datasets (over 90,000 persons), export multiple times for different time periods and specify all export IDs in the `exports` list below.\n",
    "\n",
    "This notebook combines data from two tables (0 and 2) into a small JSON file and a CSV file (same content, different formats), which is an array of objects that look like this:\n",
    "```js\n",
    "{\n",
    "    \"id\": 770826,    // crash id (used to pull the diagram)\n",
    "    \"x\": 41.73936,   // latitude\n",
    "    \"y\": -72.66404,  // longitude\n",
    "    \"k\": 1/1/2020,   // date of crash \n",
    "    \"t\": 09:41:00,   // time of crash\n",
    "    \"d\": 1593820800, // timestamp\n",
    "    \"s\": 0,          // most severe injury (K=fatal A=serious B=minor C=possible O=property)\n",
    "    \"p\": 0,          // involves a pedestrian or other pedestrian? (1=true, 0=false)\n",
    "    \"c\": 0,          // involves a cyclist or other cyclist? (1=true, 0=false)\n",
    "    \"r\": 1,          // route class (1=Interstate, 2=USRoute, 3=State, 4=Local)\n",
    "    \"to\": 0,         // total other injuries (minor) \n",
    "    \"ts\": 0,         // total serious injuries\n",
    "    \"tf\": 0          // total fatal injuries\n",
    "\n",
    "}\n",
    "```\n",
    "\n",
    "Note: the timestamp `d` needs to be multiplied by `100,000` for a proper JS timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import into Panda library\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number to match UConn data folder \n",
    "# if multiple data, then separate by commas outside single-quotes\n",
    "exports = ['50622','156713']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import Injury Status per Crash Id from table 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total injury status types for a given crash id\n",
    "total_dfs = []\n",
    "\n",
    "# Iterate through export_xxxxx_2.csv (roles) file and create new array of data only from CrashID and Injury Status\n",
    "for export in exports:\n",
    "    df = pd.read_csv(\n",
    "        f'export_{export}/export_{export}_2.csv',\n",
    "        skiprows=1,\n",
    "        usecols=['CrashId', 'Injury Status'],\n",
    "        encoding='ISO-8859-1'\n",
    "    )    \n",
    "    \n",
    "    total_dfs.append( df ) # Add two columns of data to new array \n",
    "\n",
    "totals = pd.concat(total_dfs).groupby(['CrashId'])['Injury Status'].unique() # Group data using two columns specified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Calculate Total \"Other Injuries\" per Crash Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id  to\n",
      "0       372600   2\n",
      "1       372966   2\n",
      "2       373401   2\n",
      "3       373462   1\n",
      "4       373744   0\n",
      "...        ...  ..\n",
      "30288  1034964   4\n",
      "30289  1034965   2\n",
      "30290  1035577   3\n",
      "30291  1035578   4\n",
      "30292  1035999   4\n",
      "\n",
      "[30293 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "currentId = df['CrashId'][0] # set first id in the data\n",
    "col1 = [] # List to put ids in\n",
    "col2 = [] # List to put totals in \n",
    "countO = 0\n",
    "# Loop that iterates over the ids and checks if codes that fall into \"Other Injuries\" are present \n",
    "# Total of other injuries is then summed \n",
    "for ind in df.index:\n",
    "    if df['CrashId'][ind] == currentId:\n",
    "        if df['Injury Status'][ind] == \"O\" or df['Injury Status'][ind] == \"B\" or df['Injury Status'][ind] == \"C\": # Checking injury status for codes \n",
    "            countO = countO +  1  # incrementing total for current id \n",
    "    else:\n",
    "    # Once the id changes the last id is put into the list as well as the total that is calculated\n",
    "        col1.append(df['CrashId'][ind - 1]) # Add previous id to id list\n",
    "        col2.append(countO) # Add totals summed for that id to totals list\n",
    "        currentId = df['CrashId'][ind] # Set current id to the next id once switch of id has occurred\n",
    "        if df['Injury Status'][ind] == \"O\":\n",
    "            countO = 0 # set count back to zero once id has switched\n",
    "            countO = countO +  1 # increment total for current id\n",
    "        else:\n",
    "            countO = 0 # set count back to zero \n",
    "d = {'id': col1, 'to': col2 } # add list of unique ids and totals for id together\n",
    "totalsO = pd.DataFrame(data=d) # create a data frame for two columns \n",
    "print(totalsO) # Print example of what columns are being created "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Calculate Total \"Serious Injuries\" per Crash Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentId = df['CrashId'][0]\n",
    "col1 = [] # list to store ids\n",
    "col2 = [] # list to store totals \n",
    "countS = 0 # used to increment and keep track of each id total \n",
    "\n",
    "# loop that sums total amount of \"A codes\" per unique id \n",
    "for ind in df.index:\n",
    "    if df['CrashId'][ind] == currentId:\n",
    "        if df['Injury Status'][ind] == \"A\":\n",
    "            countS = countS +  1\n",
    "    else:\n",
    "        # once crash id switches data for previous id must be stored \n",
    "        col1.append(df['CrashId'][ind - 1])\n",
    "        col2.append(countS)\n",
    "        currentId = df['CrashId'][ind]\n",
    "        if df['Injury Status'][ind] == \"A\":\n",
    "            countS = 0\n",
    "            countS = countS +  1\n",
    "        else:\n",
    "            countS = 0\n",
    "d = {'id': col1, 'ts': col2 } # add two lists together of ids and totals \n",
    "totalsS = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Calculate Total \"Fatal Injuries\" per Crash Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentId = df['CrashId'][0]\n",
    "col1 = [] # list to store ids\n",
    "col2 = []  # list to store totals\n",
    "countK = 0\n",
    "# loop that sums total amount of \"A codes\" per unique id \n",
    "for ind in df.index:\n",
    "    if df['CrashId'][ind] == currentId:\n",
    "        if df['Injury Status'][ind] == \"K\":\n",
    "            countK = countK +  1\n",
    " # once crash id switches data for previous id is stored and total for that id is stored \n",
    "    else:\n",
    "        col1.append(df['CrashId'][ind - 1])\n",
    "        col2.append(countK)\n",
    "        currentId = df['CrashId'][ind]\n",
    "        if df['Injury Status'][ind] == \"K\":\n",
    "            countK = 0\n",
    "            countK = countK +  1\n",
    "        else:\n",
    "            countK = 0\n",
    "d = {'id': col1, 'tf': col2 }\n",
    "totalsK = pd.DataFrame(data=d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import Person Type from table 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "roles_dfs = []\n",
    "\n",
    "# Iterate through export_xxxxx_2.csv (roles) file and create new array of data only from CrashID and Person Type \n",
    "for export in exports:\n",
    "    df = pd.read_csv(\n",
    "        f'export_{export}/export_{export}_2.csv',\n",
    "        skiprows=1,\n",
    "        usecols=['CrashId', 'Person Type'],\n",
    "        encoding='ISO-8859-1'\n",
    "    )\n",
    "    roles_dfs.append( df ) # Add two columns of data to new array \n",
    "\n",
    "roles = pd.concat(roles_dfs).groupby(['CrashId'])['Person Type'].unique() #Group data using two columns specified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import crashes from table 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_dfs = []\n",
    "\n",
    "for export in exports:\n",
    "    df = pd.read_csv(\n",
    "        f'export_{export}/export_{export}_0.csv',\n",
    "        skiprows=1,\n",
    "        usecols=['CrashId', 'Latitude', 'Longitude', 'Date Of Crash',\n",
    "                 'Time of Crash', 'Most Severe Injury', 'Route Class']\n",
    "    )\n",
    "    df['Date Placeholder'] = df.loc[:, 'Date Of Crash'] # Adding duplicate column of date as last column to be used for new CSV\n",
    "    df = df[['CrashId', 'Latitude', 'Longitude', 'Date Of Crash', 'Date Placeholder',\n",
    "                 'Time of Crash', 'Most Severe Injury', 'Route Class']] # Rearranging Columns so that dates are next to eachother\n",
    "    crashes_dfs.append( df ) \n",
    "    \n",
    "crashes = pd.concat(crashes_dfs) # Grouping data using 8 columns specified \n",
    "\n",
    "# Assume missing injury data refers to property injury only\n",
    "crashes['Most Severe Injury'] = crashes['Most Severe Injury'].fillna('O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Data From Exports 0 and 2 to create Smaller CSV and JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sq/n341p53s47159hhb7ybpv6180000gp/T/ipykernel_3764/1303129099.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Transforming datestamp into timestap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcrashes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrashes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcrashes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrashes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Setting up pedestrian and cyclist flags to CSV, adds a 1(true) or 0(false) for pedestrain or cyclist column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4355\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4356\u001b[0m         \"\"\"\n\u001b[0;32m-> 4357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4359\u001b[0m     def _reduce(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 \u001b[0;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m                 \u001b[0;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1099\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/var/folders/sq/n341p53s47159hhb7ybpv6180000gp/T/ipykernel_3764/1303129099.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Transforming datestamp into timestap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcrashes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrashes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcrashes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrashes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Setting up pedestrian and cyclist flags to CSV, adds a 1(true) or 0(false) for pedestrain or cyclist column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Rename columns to make JSON output smaller\n",
    "crashes.columns = ['id', 'x', 'y', 'k', 'd', 't', 's', 'r']\n",
    "\n",
    "# Transforming datestamp into timestap \n",
    "crashes.d = crashes.d.apply(lambda x: int(pd.to_datetime(x).timestamp() / 100))\n",
    "crashes.t = crashes.t.apply(lambda x: x[:-3])\n",
    "\n",
    "# Setting up pedestrian and cyclist flags to CSV, adds a 1(true) or 0(false) for pedestrain or cyclist column \n",
    "# 3 is person type code for Pedestrain, 4 is Other Pedestrain \n",
    "# 5 is person type code for Cyclist, 6 is person type code for Other Cyclist \n",
    "crashes['p'] = crashes.id.apply(lambda x: 1 if x in roles and ( 3 in roles[x] or 4 in roles[x]) else 0) \n",
    "crashes['c'] = crashes.id.apply(lambda x: 1 if x in roles and ( 5 in roles[x] or 6 in roles[x]) else 0)\n",
    "\n",
    "# Joining the three data frames that contain id and totals for injury status (other, serious and fatal). \n",
    "crashes_update1 = pd.merge(crashes, totalsO, how='left', on = 'id') # Joining Other Injury Totals with Crash Data \n",
    "crashes_update2 = pd.merge(crashes_update1, totalsS, how='left', on = 'id') # Joining Serious Injury Totals with Crash Data\n",
    "crashes_update3 = pd.merge(crashes_update2, totalsK, how='left', on = 'id')  # Joining Fatal Injury Totals with Crash Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as JSON\n",
    "crashes_update3.to_json('./crashes.json', orient='records', double_precision=5)\n",
    "\n",
    "# Save as CSV\n",
    "crashes_update3.to_csv('crashes.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_update3.head() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
